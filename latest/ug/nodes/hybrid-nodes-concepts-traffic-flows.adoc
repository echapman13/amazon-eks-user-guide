[.topic]
[#hybrid-nodes-concepts-traffic-flows]
= Network traffic flows for hybrid nodes
:info_titleabbrev: Traffic flows

[abstract]
--
Learn about the network traffic flows for EKS Hybrid Nodes.
--

This page details the network traffic flows for EKS Hybrid Nodes with diagrams showing the end-to-end network paths for the different traffic types. The following traffic flows are covered:

* <<hybrid-nodes-concepts-traffic-flows-kubelet-to-cp>>
* <<hybrid-nodes-concepts-traffic-flows-cp-to-kubelet>>
* <<hybrid-nodes-concepts-traffic-flows-pods-to-cp>>
* <<hybrid-nodes-concepts-traffic-flows-cp-to-pod>>
* <<hybrid-nodes-concepts-traffic-flows-pod-to-pod>>
* <<hybrid-nodes-concepts-traffic-flows-east-west>>

[#hybrid-nodes-concepts-traffic-flows-kubelet-to-cp]
== Hybrid node kubelet to EKS control plane

image:./images/hybrid-nodes-kubelet-to-cp-public.png[Hybrid node kubelet to EKS control plane]

=== Request

*1. Kubelet Initiates Request*

When the kubelet on a hybrid node needs to communicate with the EKS control plane (for example, to report node status or get pod specs), it uses the kubeconfig file provided during node registration. This kubeconfig contains the API server endpoint URL (the Route53 DNS name) rather than direct IP addresses.

The kubelet performs a DNS lookup for the endpoint (e.g., `https://xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx.gr7.us-west-2.eks.amazonaws.com`). In a public access cluster, this resolves to a public IP address (say `54.239.118.52`) that belongs to the EKS service running in {aws}. The kubelet then creates a secure HTTPS request to this endpoint. The initial packet looks like this:

....
+--------------------+---------------------+-----------------+
| IP Header          | TCP Header          | Payload         |
| Src: 10.80.0.2     | Src: 52390 (random) |                 |
| Dst: 54.239.118.52 | Dst: 443            |                 |
+--------------------+---------------------+-----------------+
....

*2. Local Router Routing*

Since the destination IP is a public internet address and not part of the local network, the kubelet sends this packet to its default gateway (the local on-premises router). The router examines the destination IP and determines it’s a public internet address.

For public internet traffic, the router typically forwards the packet to an internet gateway or border router that handles outbound traffic to the internet. This is ommited in the diagram and will depend on how your on-prem network is setup. The packet traverses your on-premises network infrastructure and eventually reaches your internet service provider’s network.

*3. Delivery to the EKS control plane*

The packet travels across the public internet and transit networks until it reaches {aws}’s network. {aws}’s network routes the packet to the EKS service endpoint in the appropriate region. When the packet reaches the EKS service, it’s forwarded to the actual EKS control plane for your cluster.

This routing through the public internet is different from the private VPC-routed path that we’ll see in other traffic flows. The key difference is that when using public access mode, traffic from on-premises kubelets (although not from pods) to the EKS control plane does not go through your VPC - it uses the global internet infrastructure instead.

=== Response

After the EKS control plane processes the kubelet’s request, it sends a response back:

*3. EKS control plane sends response*

The EKS control plane creates a response packet with its public IP as the source and the hybrid node’s IP as the destination:

....
+--------------------+---------------------+-----------------+
| IP Header          | TCP Header          | Payload         |
| Src: 54.239.118.52 | Src: 443            |                 |
| Dst: 10.80.0.2     | Dst: 52390          |                 |
+--------------------+---------------------+-----------------+
....

*2. Internet Routing*

The response packet travels back through the internet, following the routing path determined by internet service providers, until it reaches your on-premises network edge router.

*1. Local Delivery*

Your on-premises router receives the packet and recognizes the destination IP (10.80.0.2) as belonging to your local network. It forwards the packet through your local network infrastructure until it reaches the target hybrid node, where the kubelet receives and processes the response.

== Hybrid node kube-proxy to EKS control plane

The traffic originated from the kube-proxy on the hybrid node to the EKS control plane follows the same path as the traffic from the kubelet to the EKS control plane using the public internet (if public endpoint access is enabled for the cluster).

[#hybrid-nodes-concepts-traffic-flows-cp-to-kubelet]
== EKS control plane to hybrid node (kubelet server)

image:./images/hybrid-nodes-cp-to-kubelet.png[EKS control plane to hybrid node]

=== Request

*1. EKS Kubernetes API server initiates request*

The EKS Kubernetes API server retrieves the node’s IP address (10.80.0.2) from the node object’s status. It then routes this request through its ENI in the VPC, as the destination IP belongs to the configured remote node CIDR (10.80.0.0/16). The initial packet looks like this:

....
+-----------------+---------------------+-----------------+
| IP Header       | TCP Header          | Payload         |
| Src: 10.0.0.132 | Src: 67493 (random) |                 |
| Dst: 10.80.0.2  | Dst: 10250          |                 |
+-----------------+---------------------+-----------------+
....

*2. VPC Network Processing*

The packet leaves the ENI and enters the VPC networking layer, where it’s directed to the subnet’s gateway for further routing.

*3. VPC Route Table Lookup*

The VPC route table for the subnet containing the EKS control plane ENI has a specific route (the second one in the diagram) for the remote node CIDR. Based on this routing rule, the packet is directed to the VPC-to-onprem gateway.

*4. Cross-Boundary Transit*

The gateway transfers the packet across the cloud boundary through your established connection (Direct Connect, VPN, etc.) to your on-premises network.

*5. On-Premises Network Reception*

The packet arrives at your local on-premises router that handles traffic for the subnet where your hybrid nodes are located.

*6. Final Delivery*

The local router identifies that the destination IP (10.80.0.2) address belongs to its directly connected network and forwards the packet directly to the target hybrid node, where the kubelet server receives and processes the request.

=== Response

After the hybrid node’s kubelet processes the request, it sends back a response following the same path in reverse:

....
+-----------------+---------------------+-----------------+
| IP Header       | TCP Header          | Payload         |
| Src: 10.80.0.2  | Src: 10250          |                 |
| Dst: 10.0.0.132 | Dst: 67493          |                 |
+-----------------+---------------------+-----------------+
....

*6. Kubelet Sends Response*

The kubelet on the hybrid node (10.80.0.2) creates a response packet with the original source IP as the destination. The destination doesn’t belong to the local network so its sent to the host’s default gateway, which is the local router.

*5. Local Router Routing*

The router determines that the destination IP (10.0.0.132) belongs to 10.0.0.0/16, which has a route pointing to the gateway connecting to {aws}.

*4. Cross-Boundary Return*

The packet travels back through the same on-premises to VPC connection (Direct Connect, VPN, etc.), crossing the cloud boundary in the reverse direction.

*3. VPC Routing*

When the packet arrives in the VPC, the route tables identify that the destination IP belongs to a VPC CIDR. The packet is routed accordingly within the VPC.

*2. VPC Network Delivery*

The VPC networking layer forwards the packet to the subnet where the EKS control plane ENI (10.0.0.132) is located.

*1. ENI Reception*

The packet reaches the EKS control plane ENI attached to the Kubernetes API server, completing the round trip.

[#hybrid-nodes-concepts-traffic-flows-pods-to-cp]
== Pods running on hybrid nodes to EKS control plane

image:./images/hybrid-nodes-pod-to-cp.png[Pods running on hybrid nodes to EKS control plane]

=== Without CNI NAT-ing

=== Request

Pods generally talk to the Kubernetes API server through the `kubernetes` service. The service IP is the first IP of the cluster’s service CIDR. This convention allows pods that need to run before CoreDNS is available to reach the API server, for example, the CNI. Requests leave the pod with the service IP as the destination. For example, if the service CIDR is `172.16.0.0/16`, the service IP will be `172.16.0.1`.

*1. Pod Initiates Request*

The pod sends a request to the `kubernetes` service IP (`172.16.0.1`) on the API server port (443) from a random source port. The packet looks like this:

....
+-----------------+---------------------+-----------------+
| IP Header       | TCP Header          | Payload         |
| Src: 10.85.1.56 | Src: 67493 (random) |                 |
| Dst: 172.16.0.1 | Dst: 443            |                 |
+-----------------+---------------------+-----------------+
....

*2. CNI Processing*

The CNI detects that the destination IP doesn’t belong to any pod CIDR it manages. Since *outgoing NAT is disabled*, the CNI passes the packet to the host network stack without modifying it.

*3. Node Network Processing*

The packet enters the node’s network stack where netfilter hooks trigger the iptables rules set by kube-proxy. Several rules are applied:

. The packet first hits the `KUBE-SERVICES` chain, which contains rules matching each service’s ClusterIP and port.
. The matching rule jumps to the `KUBE-SVC-XXX` chain for the `kubernetes` service (packets destined for `172.16.0.1:443`), which contains load balancing rules. `kubernetes` service (packets destined for `172.16.0.1:443`), which contains load balancing rules.
. The load balancing rule randomly selects one of the `KUBE-SEP-XXX` chains for the CP ENI IPs (`10.0.0.132` or `10.0.1.23`).
. The selected `KUBE-SEP-XXX` chain contains the actual DNAT rule that changes the destination IP from the service IP to the selected CP ENI’s IP.

After these rules are applied, assuming that the selected EKS control plane ENI’s IP is `10.0.0.132`, the packet looks like this:

....
+-----------------+---------------------+-----------------+
| IP Header       | TCP Header          | Payload         |
| Src: 10.85.1.56 | Src: 67493 (random) |                 |
| Dst: 10.0.0.132 | Dst: 443            |                 |
+-----------------+---------------------+-----------------+
....

The node forwards the packet to its default gateway because the destination IP is not in the local network.

*4. Local Router Routing*

The local router determines that the destination IP (10.0.0.132) belongs to the VPC CIDR (10.0.0.0/16) and forwards it to the gateway connecting to {aws}.

*5. Cross-Boundary Transit*

The packet travels through your established connection (Direct Connect, VPN, etc.) across the cloud boundary to the VPC.

*6. VPC Network Delivery*

The VPC networking layer routes the packet to the correct subnet where the EKS control plane ENI (10.0.0.132) is located.

*7. ENI Reception*

The packet reaches the EKS control plane ENI attached to the Kubernetes API server.

=== Response

After the EKS control plane processes the request, it sends a response back to the pod:

*7. API Server Sends Response*

The EKS Kubernetes API server creates a response packet with the original source IP as the destination. The packet looks like this:

....
+-----------------+---------------------+-----------------+
| IP Header       | TCP Header          | Payload         |
| Src: 10.0.0.132 | Src: 443            |                 |
| Dst: 10.85.1.56 | Dst: 67493          |                 |
+-----------------+---------------------+-----------------+
....

Because the destination IP belongs to the configured remote pod CIDR (10.85.0.0/16), it sends it through its ENI in the VPC with the subnet’s router as the next hop.

*6. VPC Routing*

The VPC route table contains an entry for the remote pod CIDR (10.85.0.0/16), directing this traffic to the VPC-to-onprem gateway.

*5. Cross-Boundary Transit*

The gateway transfers the packet across the cloud boundary through your established connection (Direct Connect, VPN, etc.) to your on-premises network.

*4. On-Premises Network Reception*

The packet arrives at your local on-premises router.

*3. Delivery to node*

The router’s table has an entry for `10.85.1.0/24` with `10.80.0.2` as the next hop, delivering the packet to our node.

*2. Node Network Processing*

As the packet is processed by the node’s network stack, `conntrack` (a part of netfilter) matches the packet with the connection the pod initially establish and since DNAT was originally applied, it reverses this by rewriting the source IP from the EKS control plane ENI’s IP to the `kubernetes` service IP:

....
+-----------------+---------------------+-----------------+
| IP Header       | TCP Header          | Payload         |
| Src: 172.16.0.1 | Src: 443            |                 |
| Dst: 10.85.1.56 | Dst: 67493          |                 |
+-----------------+---------------------+-----------------+
....

*1. CNI Processing*

The CNI identifies that the destination IP belongs to a pod in its network and delivers the packet to the correct pod network namespace.

This flow showcases why Remote Pod CIDRs must be properly routable from the VPC all the way to the specific node hosting each pod - the entire return path depends on proper routing of pod IPs across both cloud and on-premises networks.

=== With CNI NAT-ing

This flow is very similar to the one _without CNI NAT-ing_, but with one key difference: the CNI applies source NAT (SNAT) to the packet before sending it to the node’s network stack. This changes the source IP of the packet to the node’s IP, allowing the packet to be routed back to the node without requiring additional routing configuration.

=== Request

*1. Pod Initiates Request*

The pod sends a request to the `kubernetes` service IP (`172.16.0.1`) on the EKS Kubernetes API server port (443) from a random source port. The packet looks like this:

....
+-----------------+---------------------+-----------------+
| IP Header       | TCP Header          | Payload         |
| Src: 10.85.1.56 | Src: 67493 (random) |                 |
| Dst: 172.16.0.1 | Dst: 443            |                 |
+-----------------+---------------------+-----------------+
....

*2. CNI Processing*

The CNI detects that the destination IP doesn’t belong to any pod CIDR it manages. Since *outgoing NAT is enabled*, the CNI applies SNAT to the packet, changing the source IP to the node’s IP before passing it to the node’s network stack:

....
+-----------------+---------------------+-----------------+
| IP Header       | TCP Header          | Payload         |
| Src: 10.80.0.2  | Src: 67493 (random) |                 |
| Dst: 172.16.0.1 | Dst: 443            |                 |
+-----------------+---------------------+-----------------+
....

Note: CNI and iptables are shown in the example as separate blocks for clarity, but in practice, it’s possible that some CNIs use iptables to apply NAT.

*3. Node Network Processing*

Here the iptables rules set by kube-proxy behave the same as in the previous example, load balancing the packet to one of the EKS control plane ENIs. The packet now looks like this:

....
+-----------------+---------------------+-----------------+
| IP Header       | TCP Header          | Payload         |
| Src: 10.80.0.2  | Src: 67493 (random) |                 |
| Dst: 10.0.0.132 | Dst: 443            |                 |
+-----------------+---------------------+-----------------+
....

The node forwards the packet to its default gateway because the destination IP is not in the local network.

*4. Local Router Routing*

The local router determines that the destination IP (10.0.0.132) belongs to the VPC CIDR (10.0.0.0/16) and forwards it to the gateway connecting to {aws}.

*5. Cross-Boundary Transit*

The packet travels through your established connection (Direct Connect, VPN, etc.) across the cloud boundary to the VPC.

*6. VPC Network Delivery*

The VPC networking layer routes the packet to the correct subnet where the EKS control plane ENI (10.0.0.132) is located.

*7. ENI Reception*

The packet reaches the EKS control plane ENI attached to the Kubernetes API server.

=== Response

After the EKS control plane processes the request, it sends a response back to the pod:

*7. API Server Sends Response*

The EKS Kubernetes API server creates a response packet with the original source IP as the destination. The packet looks like this:

....
+-----------------+---------------------+-----------------+
| IP Header       | TCP Header          | Payload         |
| Src: 10.0.0.132 | Src: 443            |                 |
| Dst: 10.80.0.2  | Dst: 67493          |                 |
+-----------------+---------------------+-----------------+
....

Because the destination IP belongs to the configured remote node CIDR (10.80.0.0/16), it sends it through its ENI in the VPC with the subnet’s router as the next hop.

*6. VPC Routing*

The VPC route table contains an entry for the remote node CIDR (10.80.0.0/16), directing this traffic to the VPC-to-onprem gateway.

*5. Cross-Boundary Transit*

The gateway transfers the packet across the cloud boundary through your established connection (Direct Connect, VPN, etc.) to your on-premises network.

*4. On-Premises Network Reception*

The packet arrives at your local on-premises router.

*3. Delivery to node*

The local router identifies that the destination IP (10.80.0.2) address belongs to its directly connected network and forwards the packet directly to the target hybrid node.

*2. Node Network Processing*

As the packet is processed by the node’s network stack, `conntrack` (a part of netfilter) matches the packet with the connection the pod initially establish and since DNAT was originally applied, it reverses this by rewriting the source IP from the EKS control plane ENI’s IP to the `kubernetes` service IP:

....
+-----------------+---------------------+-----------------+
| IP Header       | TCP Header          | Payload         |
| Src: 172.16.0.1 | Src: 443            |                 |
| Dst: 10.80.0.2  | Dst: 67493          |                 |
+-----------------+---------------------+-----------------+
....

*1. CNI Processing*

The CNI identifies this packet belongs to a connection where it has previously applied SNAT. It reverses the SNAT, changing the destination IP back to the pod’s IP:

....
+-----------------+---------------------+-----------------+
| IP Header       | TCP Header          | Payload         |
| Src: 172.16.0.1 | Src: 443            |                 |
| Dst: 10.85.1.56 | Dst: 67493          |                 |
+-----------------+---------------------+-----------------+
....

The CNI detects the destination IP belongs to a pod in its network and delivers the packet to the correct pod network namespace.

This flow showcases how CNI NAT-ing can simplify configuration by allowing packets to be routed back to the node without requiring additional routing for the pod CIDRs.

[#hybrid-nodes-concepts-traffic-flows-cp-to-pod]
== EKS control plane to pods running on a hybrid node (webhooks)

image:./images/hybrid-nodes-cp-to-pod.png[EKS control plane to pods running on a hybrid node]

This traffic pattern is most commonly seen with webhooks, where the EKS control plane needs to directly initiate connections to webhook servers running in pods on hybrid nodes. Examples include validating and mutating admission webhooks, which are called by the API server during resource validation or mutation processes.

=== Request

*1. EKS Kubernetes API server initiates request*

When a webhook is configured in the cluster and a relevant API operation triggers it, the EKS Kubernetes API server needs to make a direct connection to the webhook server pod. The API server first looks up the pod’s IP address from the Service or Endpoint resource associated with the webhook.

Assuming the webhook pod is running on a hybrid node with IP 10.85.1.23, the EKS Kubernetes API server creates an HTTPS request to the webhook’s endpoint. The initial packet is sent through the EKS control plane ENI in your VPC because the destination IP 10.85.1.23 belongs to the configured remote pod CIDR (10.85.0.0/16). The packet looks like this:

....
+-----------------+---------------------+-----------------+
| IP Header       | TCP Header          | Payload         |
| Src: 10.0.0.132 | Src: 41892 (random) |                 |
| Dst: 10.85.1.23 | Dst: 8443           |                 |
+-----------------+---------------------+-----------------+
....

*2. VPC Network Processing*

The packet leaves the EKS control plane ENI and enters the VPC networking layer with the subnet’s router as the next hop.

*3. VPC Route Table Lookup*

The VPC route table for the subnet containing the EKS control plane ENI contains a specific route for the remote pod CIDR (10.85.0.0/16). This routing rule directs the packet to the VPC-to-onprem gateway (for example, a Virtual Private Gateway for Direct Connect or VPN connections):

....
Destination     Target
10.0.0.0/16     local
10.85.0.0/16    vgw-id (VPC-to-onprem gateway)
....

*4. Cross-Boundary Transit*

The gateway transfers the packet across the cloud boundary through your established connection (Direct Connect, VPN, etc.) to your on-premises network. The packet maintains its original source and destination IP addresses as it traverses this connection.

*5. On-Premises Network Reception*

The packet arrives at your local on-premises router. The router consults its routing table to determine how to reach the 10.85.1.23 address. For this to work, your on-premises network must have routes for the pod CIDRs that direct traffic to the appropriate hybrid node.

In this case, the router’s route table contains an entry indicating that the 10.85.1.0/24 subnet is reachable via the hybrid node with IP 10.80.0.2:

....
Destination     Next Hop
10.85.1.0/24    10.80.0.2
....

*6. Delivery to node*

Based on the routing table entry, the router forwards the packet to the hybrid node (10.80.0.2). When the packet arrives at the node, it looks the same as when the EKS Kubernetes API server sent it, with the destination IP still being the pod’s IP.

*7. CNI Processing*

The node’s network stack receives the packet and, seeing that the destination IP is not the node’s own IP, passes it to the CNI for processing. The CNI identifies that the destination IP belongs to a pod running locally on this node and forwards the packet to the correct pod through the appropriate virtual interfaces:

....
Original packet → node routing → CNI → Pod's network namespace
....

The pod’s webhook server receives the request and processes it accordingly.

=== Response

After the webhook pod processes the request, it sends back a response following the same path in reverse:

*7. Pod Sends Response*

The webhook pod creates a response packet with its own IP as the source and the original requestor (the EKS control plane ENI) as the destination:

....
+-----------------+---------------------+-----------------+
| IP Header       | TCP Header          | Payload         |
| Src: 10.85.1.23 | Src: 8443           |                 |
| Dst: 10.0.0.132 | Dst: 41892          |                 |
+-----------------+---------------------+-----------------+
....

The CNI identifies that this packet is destined for an external network (not a local pod). If the CNI passes the packet to the node’s network stack with the original source IP preserved.

*6. Node Network Processing*

The node determines that the destination IP (10.0.0.132) is not in the local network and forwards the packet to its default gateway (the local router).

*5. Local Router Routing*

The local router consults its routing table and determines that the destination IP (10.0.0.132) belongs to the VPC CIDR (10.0.0.0/16). It forwards the packet to the gateway connecting to {aws}.

*4. Cross-Boundary Transit*

The packet travels back through the same on-premises to VPC connection, crossing the cloud boundary in the reverse direction.

*3. VPC Routing*

When the packet arrives in the VPC, the route tables identify that the destination IP belongs to a subnet within the VPC. The packet is routed accordingly within the VPC.

*2 & 1. EKS control plane ENI Reception*

The packet reaches the ENI attached to the EKS Kubernetes API server, completing the round trip. The API server receives the webhook’s response and continues processing the original API request based on this response.

This traffic flow demonstrates why remote pod CIDRs must be properly configured and routable: 

* The VPC must have routes for the remote pod CIDRs pointing to the on-premises gateway 
* Your on-premises network must have routes for pod CIDRs that direct traffic to the specific nodes hosting those pods 
* Without this routing configuration, webhooks and other similar services running in pods on hybrid nodes would not be reachable from the EKS control plane.

[#hybrid-nodes-concepts-traffic-flows-pod-to-pod]
== Pod-to-Pod running on hybrid nodes

image:./images/hybrid-nodes-pod-to-pod.png[Pod-to Pod running on hybrid nodes]

This section explains how pods running on different hybrid nodes communicate with each other. This example assumes your CNI uses VXLAN for encapsulation, which is common for CNIs like Cilium, Calico. The overall process is similar for other encapsulation protocols like Geneve or IP-in-IP.

=== Request

*1. Pod A Initiates Communication*

Pod A (10.85.1.56) on Node 1 wants to send traffic to Pod B (10.85.2.67) on Node 2. The initial packet looks like this:

....
+------------------+-----------------+-------------+-----------------+
| Ethernet Header  | IP Header       | TCP/UDP     | Payload         |
| Src: Pod A MAC   | Src: 10.85.1.56 | Src: 43721  |                 |
| Dst: Gateway MAC | Dst: 10.85.2.67 | Dst: 8080   |                 |
+------------------+-----------------+-------------+-----------------+
....

*2. CNI Intercepts and Processes the Packet*

When Pod A’s packet leaves its network namespace, the CNI intercepts it. The CNI consults its routing table and determines: - The destination IP (10.85.2.67) belongs to the pod CIDR - This IP is not on the local node but belongs to Node 2 (10.80.0.3) - The packet needs to be encapsulated with VXLAN.

The decision to encapsulate is critical because the underlying physical network doesn’t know how to route pod CIDRs directly - it only knows how to route traffic between node IPs.

The CNI encapsulates the entire original packet inside a VXLAN frame. This effectively creates a "`packet within a packet`" with new headers:

....
+-----------------+----------------+--------------+------------+---------------------------+
| Outer Ethernet  | Outer IP       | Outer UDP    | VXLAN      | Original Pod-to-Pod       |
| Src: Node1 MAC  | Src: 10.80.0.2 | Src: Random  | VNI: 42    | Packet (unchanged         |
| Dst: Router MAC | Dst: 10.80.0.3 | Dst: 8472    |            | from above)               |
+-----------------+----------------+--------------+------------+---------------------------+
....

Key points about this encapsulation: - The outer packet is addressed from Node 1 (10.80.0.2) to Node 2 (10.80.0.3) - UDP port 8472 is the VXLAN port Cilium uses by default - The VXLAN Network Identifier (VNI) identifies which overlay network this packet belongs to - The entire original packet (with Pod A’s IP as source and Pod B’s IP as destination) is preserved intact inside

The encapsulated packet now enters the regular networking stack of Node 1 and is processed like any other packet:

. *Node Network Processing*: Node 1’s network stack routes the packet based on its destination (10.80.0.3)
. *Local Network Delivery*:
* If both nodes are on the same Layer 2 network, the packet is sent directly to Node 2
* If they’re on different subnets, the packet is forwarded to the local router first
. *Router Handling*: The router forwards the packet based on its routing table, delivering it to Node 2

*3. Receiving Node Processing*

When the encapsulated packet arrives at Node 2 (10.80.0.3):

. The node’s network stack receives it and identifies it as a VXLAN packet (UDP port 4789)
. The packet is passed to the CNI’s VXLAN interface for processing

*4. VXLAN Decapsulation*

The CNI on Node 2 processes the VXLAN packet:

. It strips away the outer headers (Ethernet, IP, UDP, and VXLAN)
. It extracts the original inner packet
. The packet is now back to its original form:

....
+------------------+-----------------+-------------+-----------------+
| Ethernet Header  | IP Header       | TCP/UDP     | Payload         |
| Src: Pod A MAC   | Src: 10.85.1.56 | Src: 43721  |                 |
| Dst: Gateway MAC | Dst: 10.85.2.67 | Dst: 8080   |                 |
+------------------+-----------------+-------------+-----------------+
....

The CNI on Node 2 examines the destination IP (10.85.2.67) and: 

. Identifies that this IP belongs to a local pod
. Routes the packet through the appropriate virtual interfaces
. Delivers the packet to Pod B’s network namespace

=== Response

When Pod B responds to Pod A, the entire process happens in reverse:

[start=4]
. Pod B sends a packet to Pod A (10.85.1.56)
. Node 2’s CNI encapsulates it with VXLAN, setting the destination to Node 1 (10.80.0.2)
. The encapsulated packet is delivered to Node 1
. Node 1’s CNI decapsulates it and delivers the original response to Pod A

[#hybrid-nodes-concepts-traffic-flows-east-west]
== Pods on cloud nodes to pods on hybrid nodes (east-west traffic)

image:./images/hybrid-nodes-east-west.png[Pods on cloud nodes to pods on hybrid nodes]

=== Request

*1. Pod A Initiates Communication*

Pod A (10.0.0.56) on the EC2 Node wants to send traffic to Pod B (10.85.1.56) on the Hybrid Node. The initial packet looks like this:

....
+-----------------+---------------------+-----------------+
| IP Header       | TCP Header          | Payload         |
| Src: 10.0.0.56  | Src: 52390 (random) |                 |
| Dst: 10.85.1.56 | Dst: 8080           |                 |
+-----------------+---------------------+-----------------+
....

With the VPC CNI, Pod A has an IP from the VPC CIDR and is directly attached to an ENI on the EC2 instance. The pod’s network namespace is connected to the VPC network, so the packet enters the VPC routing infrastructure directly.

*2. VPC Routing*

The VPC route table contains a specific route for the Remote Pod CIDR (10.85.0.0/16), directing this traffic to the VPC-to-onprem gateway:

....
Destination     Target
10.0.0.0/16     local
10.85.0.0/16    vgw-id (VPC-to-onprem gateway)
....

Based on this routing rule, the packet is directed toward the gateway connecting to your on-premises network.

*3. Cross-Boundary Transit*

The gateway transfers the packet across the cloud boundary through your established connection (Direct Connect, VPN, etc.) to your on-premises network. The packet maintains its original source and destination IP addresses throughout this transit.

*4. On-Premises Network Reception*

The packet arrives at your local on-premises router. The router consults its routing table to determine the next hop for reaching the 10.85.1.56 address. Your on-premises router must have routes for the pod CIDRs that direct traffic to the appropriate hybrid node.

The router’s table contains an entry indicating that the 10.85.1.0/24 subnet is reachable via the hyrid node with IP 10.80.0.2:

....
Destination     Next Hop
10.85.1.0/24    10.80.0.2
....

*5. Node Network Processing*

The router forwards the packet to the Hybrid Node (10.80.0.2). When the packet arrives at the node, it still has Pod A’s IP as the source and Pod B’s IP as the destination.

*6. CNI Processing*

The node’s network stack receives the packet and, seeing that the destination IP is not its own, passes it to the CNI for processing. The CNI identifies that the destination IP belongs to a pod running locally on this node and forwards the packet to the correct pod through the appropriate virtual interfaces:

....
Original packet → node routing → CNI → Pod B's network namespace
....

Pod B receives the packet and processes it as needed.

=== Response

*6. Pod B Sends Response*

Pod B creates a response packet with its own IP as the source and Pod A’s IP as the destination:

....
+-----------------+---------------------+-----------------+
| IP Header       | TCP Header          | Payload         |
| Src: 10.85.1.56 | Src: 8080           |                 |
| Dst: 10.0.0.56  | Dst: 52390          |                 |
+-----------------+---------------------+-----------------+
....

The CNI identifies that this packet is destined for an external network and passes it to the node’s network stack.

*5. Node Network Processing*

The node determines that the destination IP (10.0.0.56) does not belong to the local network and forwards the packet to its default gateway (the local router).

*4. Local Router Routing*

The local router consults its routing table and determines that the destination IP (10.0.0.56) belongs to the VPC CIDR (10.0.0.0/16). It forwards the packet to the gateway connecting to {aws}.

*3. Cross-Boundary Transit*

The packet travels back through the same on-premises to VPC connection, crossing the cloud boundary in the reverse direction.

*2. VPC Routing*

When the packet arrives in the VPC, the routing system identifies that the destination IP belongs to a subnet within the VPC. The packet is routed through the VPC network toward the EC2 instance hosting Pod A.

*1. Pod A Receives Response*

The packet arrives at the EC2 instance and is delivered directly to Pod A through its attached ENI. Since the VPC CNI doesn’t use overlay networking for pods in the VPC, no additional decapsulation is needed - the packet arrives with its original headers intact.

This east-west traffic flow demonstrates why remote pod CIDRs must be properly configured and routable from both directions: 

* The VPC must have routes for the remote pod CIDRs pointing to the on-premises gateway 
* Your on-premises network must have routes for pod CIDRs that direct traffic to the specific nodes hosting those pods.