include::../attributes.txt[]

[.topic]
[#hybrid-nodes-concepts-kubernetes]
= Kubernetes concepts for hybrid nodes
:info_titleabbrev: Kubernetes concepts

[abstract]
--
Learn about the key Kubernetes concepts for EKS Hybrid Nodes.
--

This page details the key Kubernetes concepts that underpin the EKS Hybrid Nodes system architecture.

== EKS control plane in the VPC

The IPs of the EKS control plane ENIs are reflected in the `kubernetes` `Endpoints` object in the `default` namespace. When new ENIs are created or older ones are removed, EKS updates this object so the list of IPs is always up-to-date.

These endpoints can be accessed through the `kubernetes` Service, also in the `default` namespace. This service, of `ClusterIP` type, always gets assigned the first IP of the cluster’s service CIDR. For example, for the service CIDR `172.16.0.0/16`, the service IP will be `172.16.0.1`.

Generally, this is how pods (regardless if running in the cloud or hybrid nodes) access the EKS Kubernetes API server, using the service IP as the destination IP that gets translated to the actual IPs of one of the EKS control plane ENIs. The primary exception is kube-proxy.

== EKS API server endpoint

The `kubernetes` service IP is not the only way to access the EKS API server. EKS also creates a Route53 DNS name when you create your cluster. This is the `endpoint` field of your EKS cluster when calling the EKS DescribeCluster API.

[source,json]
----
{
    "cluster": {
        "endpoint": "https://xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx.gr7.us-west-2.eks.amazonaws.com",
        "name": "my-cluster",
        "status": "ACTIVE"
    }
}
----

In a public endpoint access or public and private endpoint access cluster, your hybrid nodes will resolve this DNS name to a public IP by default, routable through the internet. In a private endpoint access cluster, the DNS name resolves to the private IPs of the EKS control plane ENIs.

Since this is how the kubelet (and kube-proxy) access the KubernetesAPI server, if you want all your Kubernetes cluster traffic to flow through the VPC, you either need to configure your cluster in private access mode or modify your on-premises DNS server to resolve the EKS cluster endpoint to the private IPs of the EKS control plane ENIs.

== kubelet server

The kubelet’s server exposes several REST endpoints, allowing other parts of the system to interact with and gather information from each node. In most clusters, the majority of traffic to the kubelet server comes from the control plane, but certain monitoring agents may also interact with it.

Through this interface, the kubelet handles various requests: fetching logs (`kubectl logs`), executing commands inside containers (`kubectl exec`), and port-forwarding traffic (`kubectl port-forward`). Each of these requests interacts with the underlying container runtime via the kubelet, appearing seamless to cluster administrators and developers.

The most common consumer of this API is the Kubernetes API server. When you use any of the kubectl commands mentioned above, kubectl makes an API request to the API server, which then calls the kubelet API of the node where the pod is running. This is one of the main reasons why the node’s IP needs to be reachable from the EKS control plane and why, even if your pods are running, you won’t be able to access their logs or exec if the node routing is misconfigured.

*Node IPs*

When the EKS control plane communicates with a node, it uses one of the addresses reported in the `Node` object status (`status.addresses`).

With EKS cloud nodes, it’s common for the kubelet to report the private IP of the EC2 instance as an `InternalIP` during the node registration. This IP is then validated by the Cloud Controller Manager (CCM) making sure it belongs to the EC2 instance. In addition, the CCM typically adds the public IPs (as `ExternalIP`) and DNS names (`InternalDNS` and `ExternalDNS`) of the instance to the node status.

However, there is no CCM for hybrid nodes by default. When you register a hybrid node with the EKS Hybrid Nodes CLI (`nodeadm`), it configures the kubelet to not depend on a CCM and instead reports your machine’s IP directly in the node’s status.

[source,yaml]
----
apiVersion: v1
kind: Node
metadata:
  name: my-node-1
spec:
  providerID: eks-hybrid:///us-west-2/my-cluster/my-node-1
status:
  addresses:
  - address: 10.1.1.236
    type: InternalIP
  - address: my-node-1
    type: Hostname
----

If your machine has multiple IPs, the kubelet will select one of them following its own logic. You can control the selected IP with the `--node-ip` flag (which you can pass in nodeadm’s config in `spec.kubelet.flags`). Only the IP reported in the `Node` object needs to be routable from the VPC. Your machines can have other IPs that are not reachable from the cloud.

== kube-proxy

kube-proxy is responsible for implementing the Service abstraction at the networking layer of each node. It acts as a network proxy and load balancer for traffic destined to Kubernetes Services. By continuously watching the Kubernetes API server for changes related to Services and Endpoints, kube-proxy dynamically updates the underlying host’s networking rules to ensure traffic is properly directed.

In iptables mode, kube-proxy programs several netfilter chains to handle service traffic. The rules form a hierarchy:

. *KUBE-SERVICES chain*: The entry point for all service traffic. Contains rules matching each service’s ClusterIP and port.
. *KUBE-SVC-XXX chains*: Service-specific chains containing load balancing rules for each service.
. *KUBE-SEP-XXX chains*: Endpoint-specific chains containing the actual DNAT rules.

Let’s examine what happens for a service `test-server` in the `default` namespace: ++*++ Service ClusterIP: `172.16.31.14` ++*++ Service port: `80` ++*++ Backing pods: `10.2.0.110`, `10.2.1.39`, and `10.2.2.254`

When we inspect the iptables rules (using `iptables-save ++|++ grep -A10 KUBE-SERVICES`):

. In the *KUBE-SERVICES* chain, we find a rule matching the service:
+
[source,text]
----
-A KUBE-SERVICES -d 172.16.31.14/32 -p tcp -m comment --comment "default/test-server cluster IP" -m tcp --dport 80 -j KUBE-SVC-XYZABC123456
----

* This rule matches packets destined for 172.16.31.14:80
* The comment indicates what this rule is for: `default/test-server cluster IP`
* Matching packets jump to the `KUBE-SVC-XYZABC123456` chain
. The *KUBE-SVC-XYZABC123456* chain contains probability-based load balancing rules:
+
[source,text]
----
-A KUBE-SVC-XYZABC123456 -m statistic --mode random --probability 0.33333333349 -j KUBE-SEP-POD1XYZABC
-A KUBE-SVC-XYZABC123456 -m statistic --mode random --probability 0.50000000000 -j KUBE-SEP-POD2XYZABC
-A KUBE-SVC-XYZABC123456 -j KUBE-SEP-POD3XYZABC
----

* First rule: 33.3% chance to jump to `KUBE-SEP-POD1XYZABC`
* Second rule: 50% chance of the remaining traffic (33.3% of total) to jump to `KUBE-SEP-POD2XYZABC`
* Last rule: All remaining traffic (33.3% of total) jumps to `KUBE-SEP-POD3XYZABC`
. The individual *KUBE-SEP-XXX* chains perform the DNAT (Destination NAT):
+
[source,text]
----
-A KUBE-SEP-POD1XYZABC -p tcp -m tcp -j DNAT --to-destination 10.2.0.110:80
-A KUBE-SEP-POD2XYZABC -p tcp -m tcp -j DNAT --to-destination 10.2.1.39:80
-A KUBE-SEP-POD3XYZABC -p tcp -m tcp -j DNAT --to-destination 10.2.2.254:80
----
* These DNAT rules rewrite the destination IP and port to direct traffic to specific pods.
* Each rule handles approximately 33.3% of the traffic, providing even load balancing between `10.2.0.110`, `10.2.1.39` and `10.2.2.254`.

This multi-level chain structure enables kube-proxy to efficiently implement service load balancing and redirection through kernel-level packet manipulation, without requiring a proxy process in the data path.

=== Impact on Kubernetes Operations

A broken kube-proxy on a node prevents that node from routing Service traffic properly, causing timeouts or failed connections for pods that rely on cluster Services. This can be especially disruptive when a node is first registered. The CNI needs to talk to the Kubernetes API server to get information like the node’s pod CIDR before it can configure any pod networking. To do that, it uses the `kubernetes` Service IP. However, if kube-proxy has not been able to start or has failed to set the right iptables rules, the requests going to the `kubernetes` service IP won’t be sent to the actual IPs of the EKS control plane ENIs. As a consequence, the CNI will enter a crash loop and none of the pods will be able to run properly.

We know pods use the `kubernetes` service IP to communicate with the Kubernetes API server, but kube-proxy needs to first set iptables rules to make that work. 

So how does kube-proxy communicate with the API server?

The kube-proxy must be configured to use the actual IP/s of the Kubernetes API server or a DNS name that resolves to them. In the case of EKS, the default kube-proxy is configured to point to the Route53 DNS name that EKS creates when you create the cluster. You can see this value in the `kube-proxy` ConfigMap in the `kube-system` namespace. The content of this ConfigMap is a `kubeconfig` that gets injected into the kube-proxy pod, so look for the `clusters++[]++.cluster.server` field. This value will match the `endpoint` field of your EKS cluster (when calling EKS DescribeCluster API).

[source,yaml]
----
apiVersion: v1
data:
  kubeconfig: |-
    kind: Config
    apiVersion: v1
    clusters:
    - cluster:
        certificate-authority: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
        server: https://xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx.gr7.us-west-2.eks.amazonaws.com
      name: default
    contexts:
    - context:
        cluster: default
        namespace: default
        user: default
      name: default
    current-context: default
    users:
    - name: default
      user:
        tokenFile: /var/run/secrets/kubernetes.io/serviceaccount/token
kind: ConfigMap
metadata:
  name: kube-proxy
  namespace: kube-system
----

== Routable Remote Pod CIDRs

The <<hybrid-nodes-concepts-networking>> page details the requirements to run webhooks on hybrid nodes or to have pods running on cloud nodes communicate with pods running on hybrid nodes. The key requirement is that the on-premises subnet hosting your hybrid nodes needs to know which node is responsible for a particular pod IP. There are several ways to achieve this, including Border Gateway Protocol (BGP), static routes, and Address Resolution Protocol (ARP) proxying, which are covered in the following sections.

*Border Gateway Protocol (BGP)*

If your CNI supports it (like Cilium and Calico), you can use the BGP mode of your CNI to propagate routes to your per node pod CIDRs from your nodes to your local router. When using the CNI's BGP mode, your CNI acts as a virtual router, so your local router thinks the pod CIDR belongs to a different subnet and your node is the gateway to that subnet.

.bgp routing
image::./images/hybrid-nodes-bgp.png[Hybrid nodes BGP routing]

*Static routes*

Alternatively, you can configure static routes in your local router. This is the simplest way to achieve the on-premises pod CIDR routability, but it’s also the most error prone and difficult to maintain. You need to make sure that the routes are always up-to-date with the existing nodes and their assigned pod CIDRs. That said, if your number of nodes is small and infrastructure is static, this is a viable option and removes the need for BGP support in your router. If you opt for this, it is recommended to explicitly tell your CNI what is the pod CIDR slice you want to assign to each node instead of letting its IPAM decide.

.static routing
image::./images/hybrid-nodes-static-routing.png[Hybrid nodes static routing]

*Address Resolution Protocol (ARP) proxying*

ARP proxying is another approach to make on-premises pod IPs routable, particularly useful when your hybrid nodes are on the same Layer 2 network as your local router. With ARP proxying enabled, a node responds to ARP requests for pod IPs it hosts, even though those IPs belong to a different subnet.

When a device on your local network tries to reach a pod IP, it first sends an ARP request asking "`Who has this IP?`". The hybrid node hosting that pod will respond with its own MAC address, saying "`I can handle traffic for that IP.`" This creates a direct path between devices on your local network and the pods without requiring router configuration.

.arp proxying
image::./images/hybrid-nodes-arp-proxy.png[Hybrid nodes ARP proxying]

This approach has several advantages: ++*++ No need to configure your router with complex BGP or maintain static routes ++*++ Works well in environments where you don’t have control over your router configuration

For this to work, your CNI must support proxy ARP functionality. CNIs like Cilium have built-in support that can be enabled through configuration. The key consideration is that the pod CIDR must not overlap with any other network in your environment, as this could cause routing conflicts.

== Pod-to-Pod encapsulation

In on-premises environments, CNIs typically use encapsulation protocols to create overlay networks that can operate on top of the physical network without the need to re-configure it. This section explains how this encapsulation works. Note that this section is quite generic so some of the details might vary depending on the CNI you are using.

Encapsulation wraps original pod network packets inside another network packet that can be routed through the underlying physical network. This allows pods to communicate across nodes running the same CNI without requiring the physical network to know how to route those pod CIDRs.

The most common encapsulation protocol used with Kubernetes is VXLAN (Virtual Extensible LAN), though others like Geneve are also available depending on your CNI.

*VXLAN encapsulation*

VXLAN encapsulates Layer 2 Ethernet frames within UDP packets. When a pod sends traffic to another pod on a different node, the CNI performs the following:

. The packet from Pod A is intercepted by the CNI
. The CNI wraps the original packet in a VXLAN header
. This wrapped packet is then sent through the node’s regular networking
stack to the destination node
. The destination node’s CNI unwraps the packet and delivers it to Pod B

Here’s what happens to the packet structure during VXLAN encapsulation:

Original Pod-to-Pod Packet:

....
+-----------------+---------------+-------------+-----------------+
| Ethernet Header | IP Header     | TCP/UDP     | Payload         |
| Src: Pod A MAC  | Src: Pod A IP | Src Port    |                 |
| Dst: Pod B MAC  | Dst: Pod B IP | Dst Port    |                 |
+-----------------+---------------+-------------+-----------------+
....

After VXLAN Encapsulation:

....
+-----------------+-------------+--------------+------------+---------------------------+
| Outer Ethernet  | Outer IP    | Outer UDP    | VXLAN      | Original Pod-to-Pod       |
| Src: Node A MAC | Src: Node A | Src: Random  | VNI: xx    | Packet (unchanged         |
| Dst: Node B MAC | Dst: Node B | Dst: 4789    |            | from above)               |
+-----------------+-------------+--------------+------------+---------------------------+
....

The VXLAN Network Identifier (VNI) is used to distinguish between different overlay networks.

*Pod communication scenarios*

*Pods on the same hybrid node*

When pods on the same hybrid node communicate, no encapsulation is typically needed. The CNI sets up local routes that direct traffic between pods through the node’s internal virtual interfaces:

....
Pod A → veth0 → node's bridge/routing table → veth1 → Pod B
....

The packet never leaves the node and doesn’t require encapsulation.

*Pods on different hybrid nodes*

Communication between pods on different hybrid nodes requires encapsulation:

....
Pod A → CNI → [VXLAN encapsulation] → Node A network → router/gateway → Node B network → [VXLAN decapsulation] → CNI → Pod B
....

This allows the pod traffic to traverse the physical network infrastructure without requiring the physical network to understand pod IP routing.